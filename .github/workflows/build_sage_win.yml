name: Build SageAttention 2.2 (Windows • cu129 • Py3.13 • Torch 2.8 via Conda CUDA)

on:
  workflow_dispatch:
    inputs:
      sage_repo:
        description: "Upstream SageAttention repository"
        required: false
        default: "thu-ml/SageAttention"
      sage_ref:
        description: "Ref in upstream repo (branch/tag/sha). Use 'main' for 2.2 code."
        required: false
        default: "main"
      wheel_suffix:
        description: "Optional wheel filename suffix (e.g. +rev1)"
        required: false
        default: ""

permissions:
  contents: read

jobs:
  build-win:
    name: Build wheel (Win, Py 3.13.7, Torch 2.8.0+cu129, CUDA toolkit via conda)
    runs-on: windows-2022
    timeout-minutes: 75

    env:
      # Tooling / targets
      PYTHON_VERSION: "3.13.7"
      TORCH_VERSION: "2.8.0"
      TORCH_CUDA_TAG: "cu129"
      # RTX 4090 (Ada, SM 8.9). Add +PTX for forward-compat if desired.
      TORCH_CUDA_ARCH_LIST: "8.9"
      FORCE_CUDA: "1"
      # Parallelism
      EXT_PARALLEL: "4"
      NVCC_APPEND_FLAGS: "--threads 8"
      MAX_JOBS: "8"
      CMAKE_GENERATOR: "Ninja"
      EXTRA_WHEEL_SUFFIX: "${{ inputs.wheel_suffix }}"

    steps:
      # 1) Checkout your repo (workflow lives here)
      - name: Checkout current repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      # 2) Checkout the UPSTREAM SageAttention sources
      - name: Checkout SageAttention upstream
        uses: actions/checkout@v4
        with:
          repository: ${{ inputs.sage_repo }}
          ref: ${{ inputs.sage_ref }}
          path: SageAttention
          fetch-depth: 1

      - name: Show tree (debug)
        shell: bash
        run: |
          ls -la
          ls -la SageAttention

      # 3) Set up Miniconda with Python 3.13.7
      - name: Set up Miniconda (Py ${{ env.PYTHON_VERSION }})
        uses: conda-incubator/setup-miniconda@v3
        with:
          auto-update-conda: true
          activate-environment: buildenv
          python-version: ${{ env.PYTHON_VERSION }}
          channels: conda-forge,defaults
          miniforge-version: latest

      - name: Conda info
        shell: bash -l {0}
        run: |
          conda info
          python --version

      # 4) Install CUDA toolkit inside the env (provides nvcc/headers/libs).
      #    12.4 is stable on Windows runners and works with Torch cu129.
      - name: Install CUDA toolkit (conda)
        shell: bash -l {0}
        run: |
          conda install -y -c nvidia cuda-toolkit=12.4
          nvcc --version

      # 5) Build tooling
      - name: Upgrade build tooling
        shell: bash -l {0}
        run: |
          python -m pip install -U pip setuptools wheel ninja cmake packaging

      # 6) Install PyTorch 2.8.0 + cu129 (official index)
      - name: Install PyTorch ${{ env.TORCH_VERSION }} + ${{ env.TORCH_CUDA_TAG }}
        shell: bash -l {0}
        run: |
          python -m pip install --index-url https://download.pytorch.org/whl/${{ env.TORCH_CUDA_TAG }} "torch==${{ env.TORCH_VERSION }}+${{ env.TORCH_CUDA_TAG }}"
          python - << 'PY'
          import torch, sys, os
          print("Torch:", torch.__version__, "CUDA:", torch.version.cuda, "Py:", sys.version)
          print("CUDA_HOME:", os.environ.get("CUDA_HOME") or os.environ.get("CUDA_PATH") or "")
          PY

      # 7) Ensure CUDA env variables for build (Conda installs under CONDA_PREFIX)
      - name: Export CUDA env (Windows)
        shell: pwsh
        run: |
          $env:CUDA_HOME = "$env:CONDA_PREFIX"
          $env:CUDA_PATH = "$env:CONDA_PREFIX"
          $env:PATH = "$env:CONDA_PREFIX\Library\bin;$env:CONDA_PREFIX\bin;$env:PATH"
          # Persist to subsequent steps in this job
          echo "CUDA_HOME=$env:CUDA_HOME" >> $env:GITHUB_ENV
          echo "CUDA_PATH=$env:CUDA_PATH" >> $env:GITHUB_ENV
          echo "PATH=$env:PATH"         >> $env:GITHUB_ENV

      - name: Show toolchain versions
        shell: pwsh
        run: |
          python --version
          & nvcc --version
          cl 2>$null | Select-Object -First 1
          echo "CUDA_HOME=$env:CUDA_HOME"
          echo "CUDA_PATH=$env:CUDA_PATH"
          echo "TORCH_CUDA_ARCH_LIST=$env:TORCH_CUDA_ARCH_LIST"

      # 8) Build wheel (upstream uses setup.py; produce cp313-win_amd64 wheel)
      - name: Build SageAttention wheel (cp313-win_amd64)
        shell: bash -l {0}
        working-directory: SageAttention
        env:
          TORCH_CUDA_ARCH_LIST: ${{ env.TORCH_CUDA_ARCH_LIST }}
          FORCE_CUDA: ${{ env.FORCE_CUDA }}
          EXT_PARALLEL: ${{ env.EXT_PARALLEL }}
          NVCC_APPEND_FLAGS: ${{ env.NVCC_APPEND_FLAGS }}
          MAX_JOBS: ${{ env.MAX_JOBS }}
          CMAKE_GENERATOR: ${{ env.CMAKE_GENERATOR }}
        run: |
          python setup.py bdist_wheel
          ls -la dist

      # 9) Optional: add suffix to the wheel filename
      - name: Add wheel suffix (optional)
        if: ${{ env.EXTRA_WHEEL_SUFFIX != '' }}
        shell: pwsh
        working-directory: SageAttention/dist
        run: |
          $wheels = Get-ChildItem -Filter "*.whl"
          foreach ($w in $wheels) {
            $new = $w.Name -replace '\.whl$', "${{ env.EXTRA_WHEEL_SUFFIX }}.whl"
            Rename-Item -Path $w.FullName -NewName $new
            Write-Host "Renamed: $($w.Name) -> $new"
          }

      # 10) Upload the wheel
      - name: Upload wheels
        uses: actions/upload-artifact@v4
        with:
          name: sageattention-2.2-cu129-win-py313
          path: SageAttention/dist/*.whl
          if-no-files-found: error
